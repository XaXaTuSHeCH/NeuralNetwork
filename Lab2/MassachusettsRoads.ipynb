{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T05:59:38.724841Z",
     "start_time": "2025-04-16T05:59:38.717736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.utils.metrics import IoU"
   ],
   "id": "ad69be3c37abc6ca",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T17:35:39.742783Z",
     "start_time": "2025-04-15T17:35:39.736025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RoadDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, img_name.replace('.tiff', '.tif')) \n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "            mask = (mask > 0).float().unsqueeze(0)  \n",
    "        return image, mask\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.RandomCrop(height=512, width=512),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "], additional_targets={'mask': 'mask'})\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.PadIfNeeded(min_height=1504, min_width=1504, border_mode=cv2.BORDER_CONSTANT, fill=0, fill_mask=0),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "], additional_targets={'mask': 'mask'})"
   ],
   "id": "b263ba2f8f459a01",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T06:01:44.467436Z",
     "start_time": "2025-04-16T06:01:44.205305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_images = './MassachusettsRoads/train'\n",
    "train_masks = './MassachusettsRoads/train_labels'\n",
    "val_images = './MassachusettsRoads/val'\n",
    "val_masks = './MassachusettsRoads/val_labels'\n",
    "test_images = './MassachusettsRoads/test'\n",
    "test_masks = './MassachusettsRoads/test_labels'\n",
    "\n",
    "train_dataset = RoadDataset(train_images, train_masks, transform=train_transform)\n",
    "val_dataset = RoadDataset(val_images, val_masks, transform=val_transform)\n",
    "test_dataset = RoadDataset(test_images, test_masks, transform=val_transform)\n",
    "\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = smp.Unet(encoder_name=\"resnet18\", encoder_weights=\"imagenet\", classes=1, activation=None)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "iou = IoU(threshold=0.5)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ],
   "id": "2c3c71c5e501df19",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T05:50:46.644725Z",
     "start_time": "2025-04-16T05:42:06.947562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_epochs = 5\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}/{num_epochs}\", colour=\"green\"):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_iou = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=f\"Val Epoch {epoch+1}/{num_epochs}\", colour=\"red\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "            val_iou += iou(outputs, masks).item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_iou /= len(val_loader)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_weights.pth\")\n",
    "        print(f\"Model saved with Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val IoU: {val_iou:.4f}\")"
   ],
   "id": "b886e20e798ce50b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/5: 100%|\u001B[32m██████████\u001B[0m| 277/277 [01:40<00:00,  2.76it/s]\n",
      "Val Epoch 1/5: 100%|\u001B[31m██████████\u001B[0m| 4/4 [00:03<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved with Val Loss: 0.1023\n",
      "Epoch 1, Train Loss: 0.0953, Val Loss: 0.1023, Val IoU: 0.4534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2/5: 100%|\u001B[32m██████████\u001B[0m| 277/277 [01:40<00:00,  2.75it/s]\n",
      "Val Epoch 2/5: 100%|\u001B[31m██████████\u001B[0m| 4/4 [00:03<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved with Val Loss: 0.1009\n",
      "Epoch 2, Train Loss: 0.0933, Val Loss: 0.1009, Val IoU: 0.4579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3/5: 100%|\u001B[32m██████████\u001B[0m| 277/277 [01:40<00:00,  2.76it/s]\n",
      "Val Epoch 3/5: 100%|\u001B[31m██████████\u001B[0m| 4/4 [00:03<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved with Val Loss: 0.0986\n",
      "Epoch 3, Train Loss: 0.0908, Val Loss: 0.0986, Val IoU: 0.4654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4/5: 100%|\u001B[32m██████████\u001B[0m| 277/277 [01:39<00:00,  2.77it/s]\n",
      "Val Epoch 4/5: 100%|\u001B[31m██████████\u001B[0m| 4/4 [00:03<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved with Val Loss: 0.0970\n",
      "Epoch 4, Train Loss: 0.0894, Val Loss: 0.0970, Val IoU: 0.4818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5/5: 100%|\u001B[32m██████████\u001B[0m| 277/277 [01:40<00:00,  2.77it/s]\n",
      "Val Epoch 5/5: 100%|\u001B[31m██████████\u001B[0m| 4/4 [00:03<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved with Val Loss: 0.0953\n",
      "Epoch 5, Train Loss: 0.0881, Val Loss: 0.0953, Val IoU: 0.5004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T05:51:03.473528Z",
     "start_time": "2025-04-16T05:50:55.103147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "print(f\"Loaded weights from {weights_path}\")\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_iou = 0\n",
    "with torch.no_grad():\n",
    "    for images, masks in test_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, masks)\n",
    "        test_loss += loss.item()\n",
    "        test_iou += iou(outputs, masks).item()\n",
    "test_loss /= len(test_loader)\n",
    "test_iou /= len(test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test IoU: {test_iou:.4f}\")"
   ],
   "id": "542c18df72d2df4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from best_weights.pth\n",
      "Test Loss: 0.0602, Test IoU: 0.5357\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
